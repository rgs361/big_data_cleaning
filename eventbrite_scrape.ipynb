{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78107549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed77b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_links(page_number):\n",
    "    \"\"\"\n",
    "    Fetches event links from a specific page number on Eventbrite.\n",
    "    Returns a set of URLs.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.eventbrite.com/d/ny--new-york/all-events/?page={page_number}\"\n",
    "    \n",
    "    # Rotate user agents or use a standard one to look like a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        # Check if we've been redirected to page 1 (common behavior when pages run out)\n",
    "        # or if we hit a 404\n",
    "        if response.status_code == 404:\n",
    "            return set()\n",
    "        if response.url != url and \"page=1\" not in response.url and page_number != 1:\n",
    "            print(f\"  [!] Redirected to {response.url} - assuming end of pages.\")\n",
    "            return set()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        event_links = set()\n",
    "\n",
    "        # Method: JSON-LD (Structured Data) extraction\n",
    "        scripts = soup.find_all('script', type='application/ld+json')\n",
    "        \n",
    "        for script in scripts:\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "                # Helper to extract url from a single item dict\n",
    "                def extract_url(item):\n",
    "                    if 'url' in item:\n",
    "                        return item['url']\n",
    "                    elif 'item' in item and 'url' in item['item']: # Nested Schema\n",
    "                        return item['item']['url']\n",
    "                    return None\n",
    "\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        url = extract_url(item)\n",
    "                        if url: event_links.add(url)\n",
    "                \n",
    "                elif isinstance(data, dict):\n",
    "                    if 'itemListElement' in data:\n",
    "                        for item in data['itemListElement']:\n",
    "                            url = extract_url(item)\n",
    "                            if url: event_links.add(url)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        # Fallback: If JSON fails, look for 'a' tags with specific patterns\n",
    "        # Note: Eventbrite changes classes often, so checking href for '/e/' is safer\n",
    "        if not event_links:\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if '/e/' in href and 'eventbrite.com' in href:\n",
    "                    clean_link = href.split('?')[0] # Remove tracking params\n",
    "                    event_links.add(clean_link)\n",
    "                    \n",
    "        return event_links\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [!] Error fetching page {page_number}: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = set()\n",
    "page = 1\n",
    "max_safety_limit = 500  # Hard stop to prevent infinite loops if logic fails\n",
    "\n",
    "print(\"Starting scraper...\")\n",
    "\n",
    "while page <= max_safety_limit:\n",
    "    print(f\"Scraping Page {page}...\", end=\" \")\n",
    "    \n",
    "    new_links = get_event_links(page)\n",
    "    \n",
    "    # Stop condition: If no links are returned, we have likely reached the end\n",
    "    if not new_links:\n",
    "        print(\"No events found. Reached end of results.\")\n",
    "        break\n",
    "    \n",
    "    # Update master list\n",
    "    initial_count = len(all_events)\n",
    "    all_events.update(new_links)\n",
    "    new_count = len(all_events)\n",
    "    \n",
    "    print(f\"Found {len(new_links)} links. (Total unique: {new_count})\")\n",
    "    \n",
    "    # If we didn't add any new unique links, we might be seeing a 'no results' page \n",
    "    # that still has promoted links we've already seen.\n",
    "    if new_count == initial_count and page > 1:\n",
    "        print(\"  [!] No new unique links found. Stopping to avoid duplicate loops.\")\n",
    "        break\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    # PAUSE: Random sleep to be polite and avoid IP bans\n",
    "    sleep_time = random.uniform(2, 5) \n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Scraping complete. Found {len(all_events)} unique event links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b263df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eef8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "START_DATE = \"2025-12-12\"  # YYYY-MM-DD\n",
    "DAYS_TO_SCRAPE = 30        # How many days from start date\n",
    "\n",
    "# Concurrency Controls\n",
    "MAX_DAY_WORKERS = 5        # How many days to process at once (Outer Loop)\n",
    "MAX_PAGE_WORKERS = 10       # How many pages to scrape at once per day (Inner Loop)\n",
    "# NOTE: Total concurrent requests = DAY_WORKERS * PAGE_WORKERS (approx 50 here)\n",
    "\n",
    "BASE_URL = \"https://www.eventbrite.com/d/ny--new-york/all-events/\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "597c89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start_date_str, num_days):\n",
    "    \"\"\"Generates a list of date strings (YYYY-MM-DD)\"\"\"\n",
    "    start = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    return [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(num_days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a4855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_page(date, page_number):\n",
    "    \"\"\"\n",
    "    Inner Worker: Scrapes a specific page for a specific date.\n",
    "    \"\"\"\n",
    "    # Construct URL with date filters\n",
    "    params = {\n",
    "        'page': page_number,\n",
    "        'start_date': date,\n",
    "        'end_date': date\n",
    "    }\n",
    "    \n",
    "    # Random sleep to prevent \"thundering herd\" on the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': random.choice(USER_AGENTS),\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, headers=headers, timeout=10)\n",
    "        \n",
    "        # 404 or redirect usually means end of results\n",
    "        if response.status_code == 404:\n",
    "            return set()\n",
    "        if \"page=1\" in response.url and page_number > 1:\n",
    "            return set()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "\n",
    "        # 1. JSON-LD Extraction\n",
    "        scripts = soup.find_all('script', type='application/ld+json')\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if 'url' in item: links.add(item['url'])\n",
    "                elif isinstance(data, dict):\n",
    "                    if 'itemListElement' in data:\n",
    "                        for item in data['itemListElement']:\n",
    "                            if 'url' in item: links.add(item['url'])\n",
    "                            elif 'item' in item and 'url' in item['item']:\n",
    "                                links.add(item['item']['url'])\n",
    "            except: continue\n",
    "\n",
    "        # 2. Fallback Extraction\n",
    "        if not links:\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                if '/e/' in a['href'] and 'eventbrite.com' in a['href']:\n",
    "                    links.add(a['href'].split('?')[0])\n",
    "\n",
    "        return links\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"  [!] Error {date} pg {page_number}: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feb1b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_day_scrape(target_date):\n",
    "    \"\"\"\n",
    "    Outer Worker: Manages the scraping for a single day.\n",
    "    Spawns inner workers to handle pages in batches.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“… Starting Day: {target_date}\")\n",
    "    day_links = set()\n",
    "    current_page = 1\n",
    "    keep_scraping = True\n",
    "    \n",
    "    # We use a ThreadPool for the pages within this day\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PAGE_WORKERS) as page_executor:\n",
    "        \n",
    "        while keep_scraping:\n",
    "            # Create a batch of pages (e.g., try pages 1, 2, 3 concurrently)\n",
    "            # We batch to avoid queuing 50 pages for a day that has 0 events.\n",
    "            batch_size = MAX_PAGE_WORKERS\n",
    "            futures = {}\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pg = current_page + i\n",
    "                futures[page_executor.submit(scrape_single_page, target_date, pg)] = pg\n",
    "            \n",
    "            batch_has_results = False\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                page_num = futures[future]\n",
    "                try:\n",
    "                    links = future.result()\n",
    "                    if links:\n",
    "                        day_links.update(links)\n",
    "                        batch_has_results = True\n",
    "                        # print(f\"   -> {target_date} Page {page_num}: Found {len(links)} links\")\n",
    "                    else:\n",
    "                        # If a page returns empty, we might have hit the end.\n",
    "                        pass\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            # Decision Logic:\n",
    "            # If the entire batch returned 0 links, we assume the day is done.\n",
    "            # (Or if we reached a safety limit like 50 pages)\n",
    "            if not batch_has_results or current_page > 50:\n",
    "                keep_scraping = False\n",
    "            else:\n",
    "                current_page += batch_size\n",
    "                time.sleep(1) # Breath between batches\n",
    "\n",
    "    print(f\"âœ… Finished {target_date}: {len(day_links)} total events.\")\n",
    "    return day_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28555794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scrape for 30 days...\n",
      "Configuration: 5 Day-Workers x 10 Page-Workers\n",
      "ðŸ“… Starting Day: 2025-12-12\n",
      "ðŸ“… Starting Day: 2025-12-13\n",
      "ðŸ“… Starting Day: 2025-12-14\n",
      "ðŸ“… Starting Day: 2025-12-15\n",
      "ðŸ“… Starting Day: 2025-12-16\n",
      "âœ… Finished 2025-12-15: 304 total events.\n",
      "ðŸ“… Starting Day: 2025-12-17\n",
      "âœ… Finished 2025-12-16: 455 total events.\n",
      "ðŸ“… Starting Day: 2025-12-18\n",
      "âœ… Finished 2025-12-13: 700 total events.\n",
      "ðŸ“… Starting Day: 2025-12-19\n",
      "âœ… Finished 2025-12-14: 708 total events.\n",
      "ðŸ“… Starting Day: 2025-12-20\n",
      "âœ… Finished 2025-12-12: 700 total events.\n",
      "ðŸ“… Starting Day: 2025-12-21\n",
      "âœ… Finished 2025-12-17: 525 total events.\n",
      "ðŸ“… Starting Day: 2025-12-22\n",
      "âœ… Finished 2025-12-18: 570 total events.\n",
      "ðŸ“… Starting Day: 2025-12-23\n",
      "âœ… Finished 2025-12-21: 693 total events.\n",
      "ðŸ“… Starting Day: 2025-12-24\n",
      "âœ… Finished 2025-12-19: 705 total events.\n",
      "ðŸ“… Starting Day: 2025-12-25\n",
      "âœ… Finished 2025-12-20: 706 total events.\n",
      "ðŸ“… Starting Day: 2025-12-26\n",
      "âœ… Finished 2025-12-22: 188 total events.\n",
      "ðŸ“… Starting Day: 2025-12-27\n",
      "âœ… Finished 2025-12-23: 305 total events.\n",
      "ðŸ“… Starting Day: 2025-12-28\n",
      "âœ… Finished 2025-12-24: 174 total events.\n",
      "ðŸ“… Starting Day: 2025-12-29\n",
      "âœ… Finished 2025-12-25: 194 total events.\n",
      "ðŸ“… Starting Day: 2025-12-30\n",
      "âœ… Finished 2025-12-26: 456 total events.\n",
      "ðŸ“… Starting Day: 2025-12-31\n",
      "âœ… Finished 2025-12-29: 159 total events.\n",
      "ðŸ“… Starting Day: 2026-01-01\n",
      "âœ… Finished 2025-12-27: 709 total events.\n",
      "ðŸ“… Starting Day: 2026-01-02\n",
      "âœ… Finished 2025-12-30: 200 total events.\n",
      "ðŸ“… Starting Day: 2026-01-03\n",
      "âœ… Finished 2026-01-02: 172 total events.\n",
      "ðŸ“… Starting Day: 2026-01-04\n",
      "âœ… Finished 2025-12-28: 568 total events.\n",
      "ðŸ“… Starting Day: 2026-01-05\n",
      "âœ… Finished 2026-01-03: 0 total events.\n",
      "ðŸ“… Starting Day: 2026-01-06\n",
      "âœ… Finished 2025-12-31: 399 total events.\n",
      "ðŸ“… Starting Day: 2026-01-07\n",
      "âœ… Finished 2026-01-04: 0 total events.\n",
      "ðŸ“… Starting Day: 2026-01-08\n",
      "âœ… Finished 2026-01-01: 258 total events.\n",
      "ðŸ“… Starting Day: 2026-01-09\n",
      "âœ… Finished 2026-01-06: 0 total events.\n",
      "ðŸ“… Starting Day: 2026-01-10\n",
      "âœ… Finished 2026-01-05: 0 total events.\n",
      "âœ… Finished 2026-01-07: 0 total events.\n",
      "âœ… Finished 2026-01-08: 0 total events.\n",
      "âœ… Finished 2026-01-09: 0 total events.\n",
      "âœ… Finished 2026-01-10: 0 total events.\n",
      "------------------------------\n",
      "SCRAPE COMPLETE.\n",
      "Total Unique Event Links: 9250\n"
     ]
    }
   ],
   "source": [
    "all_unique_links = set()\n",
    "dates_to_scrape = get_dates(START_DATE, DAYS_TO_SCRAPE)\n",
    "\n",
    "print(f\"Starting Scrape for {len(dates_to_scrape)} days...\")\n",
    "print(f\"Configuration: {MAX_DAY_WORKERS} Day-Workers x {MAX_PAGE_WORKERS} Page-Workers\")\n",
    "\n",
    "# Outer Pool: Manages different days\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_DAY_WORKERS) as day_executor:\n",
    "    \n",
    "    # Submit all date tasks\n",
    "    future_to_date = {day_executor.submit(manage_day_scrape, date): date for date in dates_to_scrape}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_date):\n",
    "        date = future_to_date[future]\n",
    "        try:\n",
    "            links = future.result()\n",
    "            all_unique_links.update(links)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Critical failure on {date}: {e}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"SCRAPE COMPLETE.\")\n",
    "print(f\"Total Unique Event Links: {len(all_unique_links)}\")\n",
    "\n",
    "# Save to file\n",
    "with open('nyc_events_dated.txt', 'w') as f:\n",
    "    for link in all_unique_links:\n",
    "        f.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682a39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Precision Scraper...\n",
      "null\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_WORKERS = 10\n",
    "INPUT_FILE = 'nyc_events_dated.txt'\n",
    "OUTPUT_FILE = 'nyc_events_precision.json'\n",
    "\n",
    "def extract_event_precision(url):\n",
    "    time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200: return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Data Containers\n",
    "        event_info = {}\n",
    "        categories = set()\n",
    "\n",
    "        # --- STEP 1: Extract the Hydration JSON (The Truth) ---\n",
    "        # This is the massive JSON object Eventbrite uses to build the page\n",
    "        server_data_pattern = re.compile(r'window\\.__SERVER_DATA__\\s*=\\s*({.*?});', re.DOTALL)\n",
    "        script_content = soup.find('script', string=server_data_pattern)\n",
    "        \n",
    "        if script_content:\n",
    "            match = server_data_pattern.search(script_content.string)\n",
    "            if match:\n",
    "                try:\n",
    "                    full_data = json.loads(match.group(1))\n",
    "                    \n",
    "                    # Navigate to the event object (Handle standard variants)\n",
    "                    event_obj = None\n",
    "                    \n",
    "                    # Path A: Standard\n",
    "                    if 'event' in full_data: \n",
    "                        event_obj = full_data['event']\n",
    "                    # Path B: Nested in event_data\n",
    "                    elif 'event_data' in full_data and 'event' in full_data['event_data']:\n",
    "                        event_obj = full_data['event_data']['event']\n",
    "                    \n",
    "                    if event_obj:\n",
    "                        # 1. Get Title/Date/Location directly from this clean object\n",
    "                        event_info['name'] = event_obj.get('name', {}).get('text')\n",
    "                        event_info['startDate'] = event_obj.get('start', {}).get('local') # ISO String\n",
    "                        \n",
    "                        # Location usually nested\n",
    "                        if 'venue' in event_obj and event_obj['venue']:\n",
    "                            event_info['location'] = event_obj['venue'].get('name') or event_obj['venue'].get('address', {}).get('address_1')\n",
    "\n",
    "                        # --- STEP 2: Surgical Category Extraction ---\n",
    "                        \n",
    "                        # A. The Primary Category (e.g. \"Music\")\n",
    "                        # Path: event -> category -> name\n",
    "                        if 'category' in event_obj and event_obj['category']:\n",
    "                            cat_name = event_obj['category'].get('name')\n",
    "                            # Filter out 'undefined' or system names\n",
    "                            if cat_name and \"Badge\" not in cat_name:\n",
    "                                categories.add(cat_name)\n",
    "\n",
    "                        # B. The Format (e.g. \"Party\" or \"Class\")\n",
    "                        # Path: event -> format -> name\n",
    "                        if 'format' in event_obj and event_obj['format']:\n",
    "                            fmt_name = event_obj['format'].get('name')\n",
    "                            if fmt_name:\n",
    "                                categories.add(fmt_name)\n",
    "\n",
    "                        # C. The Tags (e.g. \"Cultural\", \"Hip Hop\")\n",
    "                        # Path: event -> tags -> [ { \"display_name\": \"...\" } ]\n",
    "                        if 'tags' in event_obj and isinstance(event_obj['tags'], list):\n",
    "                            for tag in event_obj['tags']:\n",
    "                                if 'display_name' in tag:\n",
    "                                    categories.add(tag['display_name'])\n",
    "                                elif 'name' in tag: # Some variants use 'name'\n",
    "                                    categories.add(tag['name'])\n",
    "                                    \n",
    "                except json.JSONDecodeError:\n",
    "                    pass # JSON parsing failed, fall back to next methods\n",
    "\n",
    "        # --- STEP 3: Fallback Normalization ---\n",
    "        # If the JSON extraction failed completely, we return partial data\n",
    "        \n",
    "        title = event_info.get('name')\n",
    "        if not title:\n",
    "             # Fallback to meta tag\n",
    "             meta_title = soup.find('meta', property='og:title')\n",
    "             title = meta_title['content'] if meta_title else \"Unknown Title\"\n",
    "\n",
    "        timestamp = None\n",
    "        start_str = event_info.get('startDate')\n",
    "        if start_str:\n",
    "            try:\n",
    "                # Eventbrite internal JSON usually gives \"2025-12-31T19:00:00\" (Local time)\n",
    "                # We assume local time for simplicity or parse timezone if available\n",
    "                dt = datetime.fromisoformat(start_str)\n",
    "                timestamp = int(dt.timestamp() * 1000)\n",
    "            except: pass\n",
    "            \n",
    "        loc_name = event_info.get('location', \"Online\")\n",
    "        \n",
    "        # --- STEP 4: Breadcrumb Backup (If JSON Categories failed) ---\n",
    "        if not categories:\n",
    "            # Look specifically for the \"Breadcrumbs\" data-testid\n",
    "            # This is safer than random links\n",
    "            breadcrumbs = soup.find('div', attrs={'data-testid': 'breadcrumbs'})\n",
    "            if breadcrumbs:\n",
    "                for link in breadcrumbs.find_all('a'):\n",
    "                    text = link.get_text(strip=True)\n",
    "                    if text not in [\"Home\", \"United States\", \"New York\", \"Events\", \"NY\"]:\n",
    "                        categories.add(text)\n",
    "\n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"DateTime\": timestamp,\n",
    "            \"Location\": loc_name,\n",
    "            \"Categories\": list(categories),\n",
    "            \"Link\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Test on the specific link that was giving \"Badge\"\n",
    "    test_links = [\n",
    "        \"https://www.eventbrite.com/e/african-dance-tickets-926482633497\",\n",
    "        \"https://www.eventbrite.com/e/new-year-eve-hottest-bollywood-desi-party-racket-nyc-tickets-1977187088807\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Running Precision Scraper...\")\n",
    "    for link in test_links:\n",
    "        result = extract_event_precision(link)\n",
    "        print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca43eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_cleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
